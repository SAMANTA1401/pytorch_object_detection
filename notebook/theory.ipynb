{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function f from a set X to a set Y is an assignment of one element of Y to each element of X. The set X is called the domain of the function and the set Y is called the codomain of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients for non-differentiable functions\n",
    "The gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable. Unfortunately many of the functions we use in practice do not have this property (relu or sqrt at 0, for example). To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:\n",
    "\n",
    "If the function is differentiable and thus a gradient exists at the current point, use it.\n",
    "\n",
    "If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).\n",
    "\n",
    "If the function is concave (at least locally), use the super-gradient of minimum norm (consider -f(x) and apply the previous point).\n",
    "\n",
    "If the function is defined, define the gradient at the current point by continuity (note that inf is possible here, for example for sqrt(0)). If multiple values are possible, pick one arbitrarily.\n",
    "\n",
    "If the function is not defined (sqrt(-1), log(-1) or most functions when the input is NaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use NaN as the gradient, but for performance reasons, some functions will use other values (log(-1), for example).\n",
    "\n",
    "If the function is not a deterministic mapping (i.e. it is not a mathematical function), it will be marked as non-differentiable. This will make it error  out in the backward if used on tensors that require grad outside of a no_grad environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/notes/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript is a powerful feature in PyTorch that allows developers to create serializable and optimizable models from PyTorch code. It serves as an intermediate representation of a PyTorch model that can be run in high-performance environments, such as C++, without the need for a Python runtime. This capability is crucial for deploying models in production environments where Python might not be available or desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is TorchScript?\n",
    "TorchScript is essentially a subset of the Python language that is specifically designed to work with PyTorch models. It allows for the conversion of PyTorch models into a format that can be executed independently of Python. This conversion is achieved through two primary methods: tracing and scripting.\n",
    "\n",
    "Tracing: This method involves running a model with example inputs and recording the operations performed. It captures the model's operations in a way that can be replayed later. However, tracing can miss dynamic control flows like loops and conditional statements because it only records the operations executed with the given inputs.\n",
    "Scripting: This method involves converting the model's source code into TorchScript. It inspects the code and compiles it into a form that can be executed by the TorchScript runtime. Scripting is more flexible than tracing as it can handle dynamic control flows, but it requires the code to be compatible with TorchScript's subset of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model and create a dummy input\n",
    "model = SimpleModel()\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# Save the traced model\n",
    "traced_model.save(\"traced_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
